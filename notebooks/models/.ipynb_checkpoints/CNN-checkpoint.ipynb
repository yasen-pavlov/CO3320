{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN based model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed packages, initialize model parameters and configure folder and file paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T12:12:26.196108Z",
     "start_time": "2019-04-27T12:12:26.188663Z"
    }
   },
   "source": [
    "Import necessary python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:43:27.906844Z",
     "start_time": "2019-05-11T11:43:26.950484Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import csv\n",
    "import shutil\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras import preprocessing, models, layers, callbacks, regularizers, utils\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from IPython.display import SVG\n",
    "\n",
    "# set tensorflow logging\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize model parameters, file and folder used by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize model related parameters. \n",
    "\n",
    "Increment the model version with every change of the model configuration. This will result in the tokenizer, and model version being saved in a seperate files that can be later loaded and reused/re-evaluated. To test and evaluate an already trained model version, change the version parameter and skip to section 4: Model Validation. \n",
    "\n",
    "Don't forget to commit the model to git after each version increment, this way the state of the notebook that was used to train the given model version can be easily reverted to.\n",
    "\n",
    "The `template_version` variable represents the version of the template notebook used as a base for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:43:28.122318Z",
     "start_time": "2019-05-11T11:43:28.114113Z"
    }
   },
   "outputs": [],
   "source": [
    "# model information\n",
    "model_name = 'CNN'\n",
    "model_version = 'final'\n",
    "train_data_version = '0.2'\n",
    "template_version = '3.0'\n",
    "\n",
    "# model parameters\n",
    "max_features = 50000\n",
    "max_words = 50\n",
    "batch_size = 256\n",
    "embedding_dimensions = 256\n",
    "CNN_size = 256\n",
    "CNN_strides = 5\n",
    "pool_size = 5\n",
    "epochs = 3\n",
    "\n",
    "# dataset file locations\n",
    "train_data_csv = '../../data/interim/sentiment140_train_clean.csv'\n",
    "test_data_csv = '../../data/interim/sentiment140_test_clean.csv'\n",
    "sts_gold_test_csv = '../../data/interim/sts-gold-clean.csv'\n",
    "\n",
    "models_folder = '../../models'\n",
    "processed_data_folder = '../../data/processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File and folder names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize file and folder names.\n",
    "\n",
    "`tensorboard_base_log_folder` is the base tensorboard log folder under which every version of the model will create its own log folder which is defined by the `tensorboard_current_log_folder` variable. This way tensorboard can be launched pointing to the base log folder and can be used to compare differences between the different versions of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:43:29.263422Z",
     "start_time": "2019-05-11T11:43:29.256661Z"
    }
   },
   "outputs": [],
   "source": [
    "# model folder and model file\n",
    "model_folder = os.path.join(models_folder, model_name)\n",
    "model_file = os.path.join(model_folder,\n",
    "                          model_name + '_' + model_version + '.h5')\n",
    "\n",
    "# tokenizer\n",
    "tokenizer_file = os.path.join(model_folder,\n",
    "                              'tokenizer_' + train_data_version + '.pickle')\n",
    "\n",
    "# tensor board log folders\n",
    "tensorboard_base_log_folder = os.path.join(model_folder, 'tensor_logs')\n",
    "tensorboard_current_log_folder = os.path.join(tensorboard_base_log_folder,\n",
    "                                              'v' + model_version)\n",
    "\n",
    "# embeddings metdata file\n",
    "embeddings_metadata_tsv_filename = 'embeddings_metadata_' + \\\n",
    "    train_data_version + '.tsv'\n",
    "tensoboard_embedddings_path = os.path.join('..', '..',\n",
    "                                           embeddings_metadata_tsv_filename)\n",
    "embeddings_metadata_tsv = os.path.join(model_folder,\n",
    "                                       embeddings_metadata_tsv_filename)\n",
    "\n",
    "# interim training data files\n",
    "train_data_file = processed_data_folder + model_name + \\\n",
    "    '_train_data_' + train_data_version + '.npy'\n",
    "train_labels_file = processed_data_folder + model_name + \\\n",
    "    '_train_labels_' + train_data_version + '.npy'\n",
    "validation_data_file = processed_data_folder + model_name + \\\n",
    "    '_validation_data_' + train_data_version + '.npy'\n",
    "validation_labels_file = processed_data_folder + model_name + \\\n",
    "    '_validation_labels_' + train_data_version + '.npy'\n",
    "\n",
    "# create model folder if it doesn't exist\n",
    "os.makedirs(model_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intialize folders, load the training dataset from disk, initialize the keras tokenizer, generate word vectors out of the texts from the training dataset and split the training dataset into a training and validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training dataset from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T13:49:39.679378Z",
     "start_time": "2019-05-04T13:49:38.186199Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train_data_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a keras tokenizer instance from disk using the `max_features` variable to define the size of the vocabulary to use for training, then serialize the fitted tokenizer to disk for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T13:50:00.066854Z",
     "start_time": "2019-05-04T13:49:39.680773Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = preprocessing.text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(df_train.text)\n",
    "\n",
    "# pickle tokenizer instance to disk\n",
    "with open(tokenizer_file, 'wb') as file:\n",
    "    pickle.dump(tokenizer, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create metadata file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tsv(tab separated values) metadata file containing the vocabulary used for the embedding layer.\n",
    "\n",
    "More info @ https://www.tensorflow.org/guide/embedding#metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T13:50:00.138778Z",
     "start_time": "2019-05-04T13:50:00.068378Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index = dict([(value, key) for (key, value) in tokenizer.word_index.items()])\n",
    "\n",
    "with open(embeddings_metadata_tsv, 'w') as file:\n",
    "    for i in range(1, max_features + 1):\n",
    "        file.write(word_index[i] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assign the polarity numpy array to the `train_labels` variable, generate sequences out of the training texts using the keras tokenizer and create word vectors to be used for training out of them which are then assigned to the `train_data` variale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T13:50:21.530992Z",
     "start_time": "2019-05-04T13:50:00.140194Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels=df_train.polarity\n",
    "train_tokenized_text = tokenizer.texts_to_sequences(df_train.text)\n",
    "train_data = preprocessing.sequence.pad_sequences(train_tokenized_text, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-25T20:20:51.415Z"
    }
   },
   "source": [
    "### Split training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the training data set into a training and validation sets, both for the word vectors and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T13:50:21.776092Z",
     "start_time": "2019-05-04T13:50:21.532313Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(train_data,\n",
    "                                                  train_labels,\n",
    "                                                  test_size=40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T21:55:47.479226Z",
     "start_time": "2019-04-27T21:55:47.474569Z"
    }
   },
   "source": [
    "### Save training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save training word vectors and labels to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T13:50:22.623236Z",
     "start_time": "2019-05-04T13:50:21.777429Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save(train_data_file, x_train)\n",
    "np.save(train_labels_file, y_train)\n",
    "np.save(validation_data_file, x_val)\n",
    "np.save(validation_labels_file, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model using keras, fit it using the training dataset and display statistics about the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T21:57:02.379572Z",
     "start_time": "2019-04-27T21:57:02.374674Z"
    }
   },
   "source": [
    "### load training data from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training word vectors and labels from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T13:50:22.741281Z",
     "start_time": "2019-05-04T13:50:22.625581Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = np.load(train_data_file)\n",
    "y_train = np.load(train_labels_file)\n",
    "x_val = np.load(validation_data_file)\n",
    "y_val = np.load(validation_labels_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model to train using the keras API\n",
    "\n",
    "initialize common parameters in section 1.2.1: Model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T13:50:22.805548Z",
     "start_time": "2019-05-04T13:50:22.742698Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-09 21:18:29.895490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1036] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-10-09 21:18:29.903372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1036] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-10-09 21:18:29.903710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1036] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-10-09 21:18:29.906363: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1036] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-10-09 21:18:29.906739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1036] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-10-09 21:18:29.907026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1036] could n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 256)         12800000  \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 256)         327936    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 13,128,193\n",
      "Trainable params: 13,128,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ot open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-10-09 21:18:30.328944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1036] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-10-09 21:18:30.329319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1036] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-10-09 21:18:30.329331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1594] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2021-10-09 21:18:30.329634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1036] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-10-09 21:18:30.329668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8946 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential(name=model_name)\n",
    "model.add(layers.Embedding(max_features, embedding_dimensions, name='embedding'))\n",
    "model.add(layers.Conv1D(CNN_size, CNN_strides, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the following callbacks:\n",
    "\n",
    "* Early stopping if the model's validation loss hasn't improved for 4 epochs\n",
    "* Model checkpoint that saves the best performing iteration to `model_file` based on validation loss\n",
    "* Tensorboard with histograms and embedding. Note in order to visualize the weights instead of the output of the embedding layer I had to revert the following keras pull request:\n",
    "https://github.com/keras-team/keras/pull/7766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T13:50:23.072517Z",
     "start_time": "2019-05-04T13:50:22.806654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
      "WARNING:tensorflow:`embeddings_layer_names` is not supported in TensorFlow 2.0. Instead, all `Embedding` layers will be visualized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-09 21:18:30.701292: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-10-09 21:18:30.701332: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-10-09 21:18:30.701363: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
      "2021-10-09 21:18:30.712829: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1666] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_NOT_INITIALIZED\n",
      "2021-10-09 21:18:30.712878: W tensorflow/core/profiler/lib/profiler_session.cc:155] Encountered error while starting profiler: Unavailable: CUPTI cannot be enabled. Another profile session might be running.\n",
      "2021-10-09 21:18:30.712911: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n"
     ]
    }
   ],
   "source": [
    "callbacks_list = [\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=4,\n",
    "    ),\n",
    "    callbacks.ModelCheckpoint(\n",
    "        filepath=model_file,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    ),\n",
    "    callbacks.TensorBoard(log_dir=tensorboard_current_log_folder,\n",
    "                          histogram_freq=1,\n",
    "                          batch_size=batch_size,\n",
    "                          embeddings_freq=1,\n",
    "                          embeddings_layer_names=['embedding'],\n",
    "                          embeddings_metadata=tensoboard_embedddings_path)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T13:37:35.076025Z",
     "start_time": "2019-04-27T13:37:35.070384Z"
    }
   },
   "source": [
    "Delete and recreate the tensorboard folder for the current version and run tensorboard on the base log folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T13:50:23.077445Z",
     "start_time": "2019-05-04T13:50:23.073792Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(tensorboard_current_log_folder, ignore_errors=True)\n",
    "os.makedirs(tensorboard_current_log_folder, exist_ok=True)\n",
    "\n",
    "# %reload_ext tensorboard.notebook\n",
    "# %tensorboard --logdir {tensorboard_base_log_folder}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T13:38:24.399059Z",
     "start_time": "2019-04-27T13:38:24.396068Z"
    }
   },
   "source": [
    "Merge the training and validations sets and fit the final version of the model on both and save to to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T13:52:43.911340Z",
     "start_time": "2019-05-04T13:50:23.078973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-09 21:18:30.881035: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-10-09 21:18:31.891425: I tensorflow/stream_executor/cuda/cuda_dnn.cc:381] Loaded cuDNN version 8204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6233/6233 [==============================] - 45s 7ms/step - loss: 0.4107 - acc: 0.8126\n",
      "Epoch 2/3\n",
      " 275/6233 [>.............................] - ETA: 41s - loss: 0.3757 - acc: 0.8322"
     ]
    }
   ],
   "source": [
    "# history = model.fit(x_train,\n",
    "#                     y_train,\n",
    "#                     epochs=epochs,\n",
    "#                     batch_size=batch_size,\n",
    "#                     callbacks=callbacks_list,\n",
    "#                     validation_data=(x_val, y_val))\n",
    "\n",
    "x_train = np.append(x_train, x_val, axis=0)\n",
    "y_train = np.append(y_train, y_val, axis=0)\n",
    "\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size)\n",
    "\n",
    "model.save(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate loss and accuracy charts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize history parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize parameters needed for the accuracy and validation charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T13:52:43.928368Z",
     "start_time": "2019-05-04T13:52:43.916256Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "#val_loss_values = history_dict['val_loss']\n",
    "accuracy_values = history_dict['acc']\n",
    "#val_accuracy_values = history_dict['val_acc']\n",
    "epochs_list = range(1, len(loss_values) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T13:52:44.078570Z",
     "start_time": "2019-05-04T13:52:43.932397Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs_list, loss_values)\n",
    "#plt.plot(epochs_list, val_loss_values)\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T14:01:31.916270Z",
     "start_time": "2019-04-27T14:01:31.910832Z"
    }
   },
   "source": [
    "Plot the training and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T13:52:44.207388Z",
     "start_time": "2019-05-04T13:52:44.080915Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs_list, accuracy_values)\n",
    "#plt.plot(epochs_list, val_accuracy_values)\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:12:38.104286Z",
     "start_time": "2019-04-27T15:12:37.945213Z"
    }
   },
   "source": [
    "Verify the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained model, tokenizer and test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T20:33:41.778732Z",
     "start_time": "2019-04-25T20:33:41.769419Z"
    }
   },
   "source": [
    "#### Load model from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:43:35.143861Z",
     "start_time": "2019-05-11T11:43:33.862867Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = models.load_model(model_file)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T20:34:00.338083Z",
     "start_time": "2019-04-25T20:34:00.335003Z"
    }
   },
   "source": [
    "#### Load tokenizer from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the prefitted tokenizer form disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:43:35.317430Z",
     "start_time": "2019-05-11T11:43:35.145221Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(tokenizer_file, 'rb') as file:\n",
    "    tokenizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load sts-gold test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:23:05.772243Z",
     "start_time": "2019-04-27T15:23:05.763715Z"
    }
   },
   "source": [
    "Load the cleaned STS-Gold dataset and convert it to word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:43:35.361097Z",
     "start_time": "2019-05-11T11:43:35.318782Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test_sts = pd.read_csv(sts_gold_test_csv)\n",
    "\n",
    "# tokenize, vectorize and pad test texts\n",
    "test_sts_labels = df_test_sts.polarity\n",
    "test_sts_tokenized_text = tokenizer.texts_to_sequences(df_test_sts.tweet)\n",
    "test_sts_data = preprocessing.sequence.pad_sequences(test_sts_tokenized_text, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load sentiment140 test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:23:05.772243Z",
     "start_time": "2019-04-27T15:23:05.763715Z"
    }
   },
   "source": [
    "Load the cleaned sentiment140 test dataset and convert it to word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:43:35.374068Z",
     "start_time": "2019-05-11T11:43:35.362475Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test_140 = pd.read_csv(test_data_csv)\n",
    "\n",
    "# tokenize, vectorize and pad test texts\n",
    "test_140_labels = df_test_140.polarity\n",
    "test_140_tokenized_text = tokenizer.texts_to_sequences(df_test_140.text)\n",
    "test_140_data = preprocessing.sequence.pad_sequences(test_140_tokenized_text, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the model's performance against the test datasets and test it with example phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T00:12:39.765142Z",
     "start_time": "2019-04-29T00:12:39.762257Z"
    }
   },
   "source": [
    "#### STS-Gold dataset test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:25:35.423483Z",
     "start_time": "2019-04-27T15:25:35.415152Z"
    }
   },
   "source": [
    "Evaluate the model against the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:43:36.698891Z",
     "start_time": "2019-05-11T11:43:35.375362Z"
    }
   },
   "outputs": [],
   "source": [
    "predict_sts = model.predict(test_sts_data) \n",
    "sts_predict_classes = (predict_sts > 0.5).astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STS-Gold Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and display the evaluation metrics for the STS-Gold dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:43:36.710614Z",
     "start_time": "2019-05-11T11:43:36.700480Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(test_sts_labels, sts_predict_classes)\n",
    "precision = metrics.precision_score(test_sts_labels, sts_predict_classes)\n",
    "recall = metrics.recall_score(test_sts_labels, sts_predict_classes)\n",
    "f1 = metrics.f1_score(test_sts_labels, sts_predict_classes)\n",
    "roc_auc = metrics.roc_auc_score(test_sts_labels, sts_predict_classes)\n",
    "cohen_kappa = metrics.cohen_kappa_score(test_sts_labels, sts_predict_classes)\n",
    "\n",
    "print('Accuracy:\\t{:.2%}'.format(accuracy))\n",
    "print('Precision:\\t{:.2%}'.format(precision))\n",
    "print('Recall:\\t\\t{:.2%}'.format(recall))\n",
    "print('F1:\\t\\t{:.2%}'.format(f1))\n",
    "print('ROC AUC:\\t{:.2%}'.format(roc_auc))\n",
    "print('Cohen\\'s kappa:\\t{:.2%}'.format(cohen_kappa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STS-Gold Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:43:36.793479Z",
     "start_time": "2019-05-11T11:43:36.711653Z"
    }
   },
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(test_sts_labels, sts_predict_classes)\n",
    "fig, ax = plot_confusion_matrix(conf_mat=confusion_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T00:36:21.683674Z",
     "start_time": "2019-04-29T00:36:21.671990Z"
    }
   },
   "source": [
    "Calculate and display the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T00:12:39.765142Z",
     "start_time": "2019-04-29T00:12:39.762257Z"
    }
   },
   "source": [
    "#### Sentiment140 dataset test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:25:35.423483Z",
     "start_time": "2019-04-27T15:25:35.415152Z"
    }
   },
   "source": [
    "Evaluate the model against the Sentiment140 test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:43:36.843976Z",
     "start_time": "2019-05-11T11:43:36.794686Z"
    }
   },
   "outputs": [],
   "source": [
    "predict_s140 = model.predict(test_140_data) \n",
    "s140_predict_classes = (predict_s140 > 0.5).astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiment140 Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and display the evaluation metrics for the Sentiment140 test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:43:36.854832Z",
     "start_time": "2019-05-11T11:43:36.845927Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(test_140_labels, s140_predict_classes)\n",
    "precision = metrics.precision_score(test_140_labels, s140_predict_classes)\n",
    "recall = metrics.recall_score(test_140_labels, s140_predict_classes)\n",
    "f1 = metrics.f1_score(test_140_labels, s140_predict_classes)\n",
    "roc_auc = metrics.roc_auc_score(test_140_labels, s140_predict_classes)\n",
    "cohen_kappa = metrics.cohen_kappa_score(test_140_labels, s140_predict_classes)\n",
    "\n",
    "print('Accuracy:\\t{:.2%}'.format(accuracy))\n",
    "print('Precision:\\t{:.2%}'.format(precision))\n",
    "print('Recall:\\t\\t{:.2%}'.format(recall))\n",
    "print('F1:\\t\\t{:.2%}'.format(f1))\n",
    "print('ROC AUC:\\t{:.2%}'.format(roc_auc))\n",
    "print('Cohen\\'s kappa:\\t{:.2%}'.format(cohen_kappa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Sentiment140 Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T00:36:21.683674Z",
     "start_time": "2019-04-29T00:36:21.671990Z"
    }
   },
   "source": [
    "Calculate and display the confusion matrix for the Sentiment140 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:43:36.932124Z",
     "start_time": "2019-05-11T11:43:36.855867Z"
    }
   },
   "outputs": [],
   "source": [
    "confusion_matrix = metrics.confusion_matrix(test_140_labels, s140_predict_classes)\n",
    "fig, ax = plot_confusion_matrix(conf_mat=confusion_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T00:45:26.476314Z",
     "start_time": "2019-04-29T00:45:26.471650Z"
    }
   },
   "source": [
    "#### Sample predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T00:50:32.349171Z",
     "start_time": "2019-04-29T00:50:32.341569Z"
    }
   },
   "source": [
    "Define a predict function and test the model on a few sample sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a helper function that takes a string and returns a sentiment prediction for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:43:36.937950Z",
     "start_time": "2019-05-11T11:43:36.934504Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    word_vector = preprocessing.sequence.pad_sequences(sequence, maxlen=max_words)\n",
    "    prediction = model.predict(word_vector)\n",
    "    return '{:.2%}'.format((prediction)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model on a sample of phrases to validate the model's performance when it comes to different language constructs like negation, modal verbs and sarcasm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:43:36.984432Z",
     "start_time": "2019-05-11T11:43:36.940308Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phrases = [\n",
    "    \"This movie was horribly good\", \n",
    "    \"I love to hate that book\",\n",
    "    \"I hate to admit that he is great\",\n",
    "    \"You don't say\",\n",
    "    \"Yesterday was awfully bad\",\n",
    "    \"On a second thought it was amazingly bad\",\n",
    "    \"Tell me something I don’t know\",\n",
    "    \"This can't get any better\"\n",
    "]\n",
    "\n",
    "df_phrases_sentiment = pd.DataFrame(phrases, columns=['phrases'])\n",
    "df_phrases_sentiment['sentiment'] = df_phrases_sentiment.phrases.map(predict)\n",
    "\n",
    "df_phrases_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architectural diagram of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-11T11:45:59.710685Z",
     "start_time": "2019-05-11T11:45:59.584040Z"
    }
   },
   "outputs": [],
   "source": [
    "SVG(utils.vis_utils.model_to_dot(model, show_shapes=True,\n",
    "                                 show_layer_names=False).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "634px",
    "left": "901px",
    "top": "226px",
    "width": "294px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 1115,
   "position": {
    "height": "40px",
    "left": "878px",
    "right": "20px",
    "top": "120px",
    "width": "647px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
