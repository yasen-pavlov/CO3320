{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake Project Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:46:04.464860Z",
     "start_time": "2019-04-27T15:46:04.462208Z"
    }
   },
   "source": [
    "Preparation of the Fake Project dataset.\n",
    "\n",
    "The dataset can be downloaded from:\n",
    "https://botometer.iuni.iu.edu/bot-repository/datasets/cresci-2017/cresci-2017.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import python packages, initialize parameters and load datasets from csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T09:18:51.947165Z",
     "start_time": "2019-05-07T09:18:51.447262Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize variables for the dataset headers and, input and output file locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T09:18:59.005851Z",
     "start_time": "2019-05-07T09:18:59.000490Z"
    }
   },
   "outputs": [],
   "source": [
    "user_dataset_csv = '../../data/external/cresci-2017/genuine_accounts/tweets.csv'\n",
    "social_bot_1_dataset_csv = '../../data/external/cresci-2017/social_spambots_1/tweets.csv'\n",
    "social_bot_2_dataset_csv = '../../data/external/cresci-2017/social_spambots_2/tweets.csv'\n",
    "social_bot_3_dataset_csv = '../../data/external/cresci-2017/social_spambots_3/tweets.csv'\n",
    "\n",
    "columns_to_drop = [\n",
    "    \"id\", \"source\", \"user_id\", \"truncated\", \"in_reply_to_status_id\",\n",
    "    \"in_reply_to_user_id\", \"in_reply_to_screen_name\", \"retweeted_status_id\",\n",
    "    \"geo\", \"place\", \"contributors\", \"retweet_count\", \"reply_count\",\n",
    "    \"favorite_count\", \"favorited\", \"retweeted\", \"possibly_sensitive\",\n",
    "    \"num_hashtags\", \"num_urls\", \"num_mentions\", \"created_at\", \"timestamp\",\n",
    "    \"crawled_at\", \"updated\"\n",
    "]\n",
    "\n",
    "train_clean_csv = '../../data/interim/fake_project_train_clean.csv'\n",
    "test_clean_csv = '../../data/interim/fake_project_test_clean.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-30T11:45:09.959191Z",
     "start_time": "2019-04-30T11:45:09.957131Z"
    }
   },
   "source": [
    "## Prepare datasets|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load datasets, create type columns and split them into training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and test datasets from disk and drop the unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T09:19:30.740815Z",
     "start_time": "2019-05-07T09:19:01.480762Z"
    }
   },
   "outputs": [],
   "source": [
    "df_user = pd.read_csv(user_dataset_csv, low_memory=False)\n",
    "df_bot1 = pd.read_csv(social_bot_1_dataset_csv, low_memory=False)\n",
    "df_bot2 = pd.read_csv(social_bot_2_dataset_csv, low_memory=False)\n",
    "df_bot3 = pd.read_csv(social_bot_3_dataset_csv, low_memory=False)\n",
    "\n",
    "df_user.drop(columns_to_drop, axis=1, inplace=True)\n",
    "df_bot1.drop(columns_to_drop, axis=1, inplace=True)\n",
    "df_bot2.drop(columns_to_drop, axis=1, inplace=True)\n",
    "df_bot3.drop(columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "df_bot = df_bot1.append(df_bot2.append(df_bot3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tweet type columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column in both dataframes signifying the type of the tweet: 0 for normal user and 1 for content polluter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T09:19:33.384775Z",
     "start_time": "2019-05-07T09:19:33.370138Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3457133 entries, 0 to 1418556\n",
      "Data columns (total 1 columns):\n",
      "text    object\n",
      "dtypes: object(1)\n",
      "memory usage: 52.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_bot.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T09:19:58.438958Z",
     "start_time": "2019-05-07T09:19:58.423803Z"
    }
   },
   "outputs": [],
   "source": [
    "df_user['type'] = 0\n",
    "df_bot['type'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split datasets into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T09:20:03.449573Z",
     "start_time": "2019-05-07T09:20:02.156373Z"
    }
   },
   "outputs": [],
   "source": [
    "df_user_train, df_user_test = train_test_split(df_user, test_size=20000)\n",
    "df_bot_train, df_bot_test = train_test_split(df_bot, test_size=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the user and bot dataframes into 1 for both the training and the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T09:20:09.727191Z",
     "start_time": "2019-05-07T09:20:09.566724Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = df_bot_train.append(df_user_train)\n",
    "df_test = df_bot_test.append(df_user_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cleanup steps will be applied on the datasets\n",
    "\n",
    "1. Convert to lowercase\n",
    "3. tokenkize using nltk's twitter tokenizer\n",
    "4. replace all http links with the string 'http'\n",
    "5. Filter out all mentions tokens\n",
    "6. Filter out all hashtags tokens\n",
    "7. Filter out all tokens containing non-letter characters\n",
    "8. Join tokens back together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process text function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that will be applied to all texts in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T09:20:14.188057Z",
     "start_time": "2019-05-07T09:20:14.183023Z"
    }
   },
   "outputs": [],
   "source": [
    "#create a set of all lowercase ascii character plus \"'\"\n",
    "letters = set(string.ascii_lowercase + \"'\")\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def process_text(text):\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = ['http' if t.startswith('http') else t for t in tokens]\n",
    "    tokens = list(filter(lambda t: not t.startswith('@'), tokens))\n",
    "    tokens = list(filter(lambda t: not t.startswith('#'), tokens))\n",
    "    tokens = list(filter(lambda t: set(t).issubset(letters), tokens))\n",
    "    tokens = list(filter(lambda t: not t == \"'\", tokens))\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the function on all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T09:25:01.487261Z",
     "start_time": "2019-05-07T09:20:44.278751Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6256495/6256495 [04:14<00:00, 24546.32it/s]\n",
      "100%|██████████| 40000/40000 [00:01<00:00, 24378.93it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train.text = df_train.text.progress_map(process_text)\n",
    "df_test.text = df_test.text.progress_map(process_text)\n",
    "df_train = df_train[df_train.text!='']\n",
    "df_test = df_test[df_test.text!='']\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save clean datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the cleaned up datasets back to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T09:26:50.998647Z",
     "start_time": "2019-05-07T09:26:38.185951Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.to_csv(train_clean_csv, index=False)\n",
    "df_test.to_csv(test_clean_csv, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "634px",
    "left": "1081px",
    "top": "54px",
    "width": "277px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
