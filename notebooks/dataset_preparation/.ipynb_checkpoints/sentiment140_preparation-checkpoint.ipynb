{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment140 dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T15:46:04.464860Z",
     "start_time": "2019-04-27T15:46:04.462208Z"
    }
   },
   "source": [
    "Clean and prepare the Stanford Sentiment140 Datastet.\n",
    "\n",
    "The training dataset contains 1 600 000 tweets split equally between positive and negative ones.\n",
    "\n",
    "The dataset can be downloaded from:\n",
    "http://help.sentiment140.com/for-students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import python packages, initialize parameters and load datasets from csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T11:04:34.917451Z",
     "start_time": "2019-05-15T11:04:34.423579Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize variables for the dataset headers and, input and output file locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T11:04:34.921283Z",
     "start_time": "2019-05-15T11:04:34.918820Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset_csv = '../../data/external/training.1600000.processed.noemoticon.csv'\n",
    "test_dataset_csv = '../../data/external/testdata.manual.2009.06.14.csv'\n",
    "dataset_headers = ['polarity', 'id', 'date', 'query', 'user', 'text']\n",
    "\n",
    "train_clean_csv= '../../data/interim/sentiment140_train_clean.csv'\n",
    "test_clean_csv= '../../data/interim/sentiment140_test_clean.csv'\n",
    "test_txt = '../../reports/sentiment140_test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training and test datasets from disk and drop the unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T11:04:37.734879Z",
     "start_time": "2019-05-15T11:04:34.922699Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(train_dataset_csv, header=None, names=dataset_headers)\n",
    "df_test = pd.read_csv(test_dataset_csv, header=None, names=dataset_headers)\n",
    "df_train.drop(['id', 'date', 'query', 'user'], axis=1, inplace=True)\n",
    "df_test.drop(['id', 'date', 'query', 'user'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remap polarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remap the positive sentiment polarity from 4 to 1 and drop the neutral polarity from the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T11:04:37.775983Z",
     "start_time": "2019-05-15T11:04:37.736266Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test = df_test[df_test.polarity!=2]\n",
    "df_train.polarity = df_train.polarity.map({0:0, 4:1})\n",
    "df_test.polarity = df_test.polarity.map({0:0, 4:1}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cleanup steps will be applied on the datasets\n",
    "\n",
    "1. Convert to lowercase\n",
    "2. decode all html encoded symbols\n",
    "3. tokenkize using nltk's twitter tokenizer\n",
    "4. Filter out all http link tokens\n",
    "5. Filter out all mentions tokens\n",
    "6. Filter out all hashtags tokens\n",
    "7. Filter out all tokens containing non-letter characters\n",
    "8. Join tokens back together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process text function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that will be applied to all texts in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T11:04:37.780845Z",
     "start_time": "2019-05-15T11:04:37.777331Z"
    }
   },
   "outputs": [],
   "source": [
    "#create a set of all lowercase ascii character plus \"'\"\n",
    "letters = set(string.ascii_lowercase + \"'\")\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def process_text(text):\n",
    "    text = text.lower()\n",
    "    text = BeautifulSoup(text, 'lxml').get_text()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = list(filter(lambda t: not t.startswith('http'), tokens))\n",
    "    tokens = list(filter(lambda t: not t.startswith('@'), tokens))\n",
    "    tokens = list(filter(lambda t: not t.startswith('#'), tokens))\n",
    "    tokens = list(filter(lambda t: set(t).issubset(letters), tokens))\n",
    "    tokens = list(filter(lambda t: not t == \"'\", tokens))\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the function on all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T11:09:51.183713Z",
     "start_time": "2019-05-15T11:04:37.782011Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600000/1600000 [05:13<00:00, 5110.36it/s]\n",
      "100%|██████████| 359/359 [00:00<00:00, 4974.56it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train.text = df_train.text.progress_map(process_text)\n",
    "df_test.text = df_test.text.progress_map(process_text)\n",
    "df_train = df_train[df_train.text!='']\n",
    "df_test = df_test[df_test.text!='']\n",
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save clean datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the cleaned up datasets back to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T11:09:55.111648Z",
     "start_time": "2019-05-15T11:09:51.185025Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.to_csv(train_clean_csv, index=False)\n",
    "df_test.to_csv(test_clean_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate benchmark file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input file for SentiStrength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T11:09:55.130608Z",
     "start_time": "2019-05-15T11:09:55.115908Z"
    }
   },
   "outputs": [],
   "source": [
    "df_bench = pd.read_csv(test_dataset_csv, header=None, names=dataset_headers)\n",
    "df_bench = df_bench[df_bench.polarity!=2]\n",
    "np.savetxt(test_txt, df_bench.text, fmt=\"%s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "634px",
    "left": "1081px",
    "top": "54px",
    "width": "277px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
